{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {},
   "source": [
    "---\n",
    "title: Load and Display Input Data for Discrete Time Simulation Model\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-overflow: wrap\n",
    "    embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization tools.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Has multi-dimensional arrays and matrices. Has a large collection of\n",
    "# mathematical functions to operate on these arrays.\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Data manipulation and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "# for survival analysis\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "\n",
    "# to be able to clear output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Enables autoreloading of specified modules.\n",
    "%load_ext autoreload \n",
    "%autoreload 1\n",
    "\n",
    "# import the allocation model\n",
    "from kidney_dtsim.model import *\n",
    "from kidney_dtsim.helper_functions import *\n",
    "\n",
    "# specify model to be autoreloaded\n",
    "%aimport kidney_dtsim\n",
    "\n",
    "# if data \n",
    "try:\n",
    "    data_path # type: ignore\n",
    "except NameError:\n",
    "    data_path = \"../model_input_public/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "We load the data for the patients listed 2006-01-01. In the next step we will use this data, to add the initial patients to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active_waiting_2006 = pd.read_csv(data_path + \"tidy_waiting_list_example.csv\", sep = \";\", decimal=\",\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Predicting waiting list removal probability\n",
    "First we load the data for the age at removal from the waiting list.\n",
    "Removal from waiting list is considered an event. Patients were censored, if transplanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removal_surv = pd.read_csv(data_path + \"removal_survival.csv\",\n",
    "                              sep=\";\", index_col=0, decimal=\",\")\n",
    "df_removal_surv.dropna(inplace=True)\n",
    "df_removal_surv[df_removal_surv[\"event_time\"] < 0]\n",
    "df_removal_surv.drop(labels= df_removal_surv[df_removal_surv[\"event_time\"] < 0].index, axis=\"index\", inplace=True)\n",
    "df_removal_surv.loc[df_removal_surv[\"event_time\"] <= 0, \"event_time\"] += 1\n",
    "\n",
    "df_removal_surv[\"no_zero_time_dial_to_registration\"] = df_removal_surv[\"time_dial_to_registration\"].apply(lambda x: 1 if x <= 0 else x)\n",
    "df_removal_surv[\"sqrt_time_dial_to_registration\"] = np.sqrt(df_removal_surv[\"no_zero_time_dial_to_registration\"])\n",
    "\n",
    "def assign_age_group(age):\n",
    "    if age <= 29:\n",
    "        return \"0-29\"\n",
    "    elif age <= 39:\n",
    "        return \"30-39\"\n",
    "    elif age <= 44:\n",
    "        return \"40-44\"\n",
    "    elif age <= 49:\n",
    "        return \"45-49\"\n",
    "    elif age <= 54:\n",
    "        return \"50-54\"\n",
    "    elif age <= 59:\n",
    "        return \"55-59\"\n",
    "    elif age <= 64:\n",
    "        return \"60-64\"\n",
    "    elif age <= 69:\n",
    "        return \"65-69\"\n",
    "    elif age <= 74:\n",
    "        return \"70-74\"\n",
    "    elif age <= 79:\n",
    "        return \"75-79\"\n",
    "    else:\n",
    "        return \"80+\"\n",
    "\n",
    "df_removal_surv[\"age_group\"] = df_removal_surv[\"age_at_reg_waiting_list\"].apply(assign_age_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Fit a Cox Proportional Hazards Model with parametric baseline\n",
    "\n",
    "A Cox Proportional Hazards model was selected, with a parametric baseline fitted with cubic splines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cph_spline = CoxPHFitter(baseline_estimation_method=\"spline\", n_baseline_knots=4)\n",
    "cph_spline.fit(df_removal_surv, duration_col=\"event_time\", event_col=\"event\", entry_col=\"entry_time\", strata = \"age_group\", formula=\"no_zero_time_dial_to_registration + sqrt_time_dial_to_registration\")\n",
    "\n",
    "cph_spline.print_summary(style='ascii') # if not style ascii the formating is lost in the rendered document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We compare the model to the Kapplan-Meier Estimate. Shown is the 'survival' probability, which means the probability of not being removed from the waiting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_to_kmf(model, df):\n",
    "    # Plot Kaplane-Meier Fitter\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(durations=df[\"event_time\"], event_observed=df[\"event\"], entry=df[\"entry_time\"])\n",
    "    ax = plt.subplot()\n",
    "    kmf.plot_survival_function(ax=ax)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Plot other Survival Model\n",
    "    model.predict_survival_function(df_removal_surv).mean(axis=1).plot(ax=ax)\n",
    "    \n",
    "    ax.set_xlabel(\"days on waiting list\")\n",
    "    ax.set_ylabel(\"Waiting list 'persistence' probability\")\n",
    "    ax.legend([ax.get_lines()[0], ax.get_lines()[1]], [\"Kaplan-Meier Estimate\", type(model).__name__ ])\n",
    "    ax.set_xlim(0, 11000)\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "    add_at_risk_counts(kmf, ax = ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_model_to_kmf(cph_spline, df_removal_surv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Non transplantable and transplantable status probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nt_status_prob = pd.read_csv(data_path + \"nt_status_prob_aalen_johansen.csv\", sep = \";\", index_col = 0)\n",
    "\n",
    "df_nt_status_prob.index = pd.to_numeric(df_nt_status_prob.index, errors='coerce')\n",
    "df_nt_status_prob['pstate.not_transplantable'] = pd.to_numeric(df_nt_status_prob['pstate.not_transplantable'].str.replace(',', '.'), errors='coerce')\n",
    "\n",
    "print(df_nt_status_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Load Odds Ratios for organ acceptance\n",
    "We load odds ratios for organ acceptance of patients in the ETKAS program. The odds ratios are derived from a dataset by Eurotransplant. The logistic regression model is a piecewise logistic regression model.\n",
    "\n",
    "For the odds-ratio calculation only kidney placed through the regular ETKAS program were evaluated. Candidates whose profile indicated, they didn't want to be offered that graft, were filtered out. The calculation was performed on data from 2016 - 2020 for the ETKAS program and on data from 2014-2019 for the ESP program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_odds_ETKAS = pd.read_csv(data_path + \"odds_ratios_rd_ETKAS_filtered.csv\")\n",
    "df_log_odds_ETKAS[\"level\"] = df_log_odds_ETKAS[\"level\"].apply(lambda x: str(int(x)) if not pd.isna(x) else \"\")\n",
    "df_log_odds_ETKAS[\"var_name\"] = df_log_odds_ETKAS[\"variable\"].fillna('').astype(str) + df_log_odds_ETKAS[\"variable_transformation\"].fillna('').astype(str) + df_log_odds_ETKAS[\"level\"]\n",
    "\n",
    "dict_log_odds_ETKAS = df_log_odds_ETKAS.set_index(\"var_name\")[\"coef\"].to_dict()\n",
    "print(dict_log_odds_ETKAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_odds_ESP = pd.read_csv(data_path + \"odds_ratios_rd_ESP_filtered_modern.csv\")\n",
    "df_log_odds_ESP[\"level\"] = df_log_odds_ESP[\"level\"].apply(lambda x: str(int(x)) if not pd.isna(x) else \"\")\n",
    "df_log_odds_ESP[\"var_name\"] = df_log_odds_ESP[\"variable\"].fillna('').astype(str) + df_log_odds_ESP[\"variable_transformation\"].fillna('').astype(str) + df_log_odds_ESP[\"level\"]\n",
    "\n",
    "dict_log_odds_ESP = df_log_odds_ESP.set_index(\"var_name\")[\"coef\"].to_dict()\n",
    "print(dict_log_odds_ESP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Load HLA frequencies\n",
    "\n",
    "The HLA frequencies are derived from http://www.allelefrequencies.net/pop6001c.asp?pop_id=3089 . The selected population was \"Germany pop 8\".\n",
    "- Sample size: 39689 (A = 39654, B = 39496, DRB1 = 33683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_a_freq = pd.read_csv(data_path + \"hla_a_freq.csv\")\n",
    "hla_a_freq = hla_a_freq.set_index(\"match_broad\")[\"freq\"].to_dict()\n",
    "print(hla_a_freq)\n",
    "\n",
    "hla_b_freq = pd.read_csv(data_path + \"hla_b_freq.csv\")\n",
    "hla_b_freq = hla_b_freq.set_index(\"match_broad\")[\"freq\"].to_dict()\n",
    "print(hla_b_freq)\n",
    "\n",
    "hla_dr_freq = pd.read_csv(data_path + \"hla_drb1_freq.csv\")\n",
    "hla_dr_freq = hla_dr_freq.set_index(\"match_split\")[\"freq\"].to_dict()\n",
    "print(hla_dr_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Load HLA types\n",
    "\n",
    "The HLA hla types are derived from the supplementary files of the following article: Seitz S, Lange V, Norman PJ, Sauter J, Schmidt AH. Estimating HLA haplotype frequencies from homozygous individuals - A Technical Report. Int J Immunogenet. 2021 Dec;48(6):490-495. doi: 10.1111/iji.12553. Epub 2021 Sep 27. PMID: 34570965\n",
    "- Sample size: 3456066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hla_haplo_freq = pd.read_csv(data_path + \"hla_haplo_freq_split.csv\", index_col=0)\n",
    "print(df_hla_haplo_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "On the waiting list are cases, where the hla type is only partially reported or not reported at all. The missing hla types are imputed based on the haplotype frequencies in germany. If the hla type is only partially reported or unacceptable antigens are reported, the haplotype frequencies are first filtered for the reported HLAs and for haplotypes not present in the unacceptable antigens for that patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_hla_haplotype(df,\n",
    "                      a_split=None,\n",
    "                      a_broad=None,\n",
    "                      b_split=None,\n",
    "                      b_broad=None,\n",
    "                      c_split=None,\n",
    "                      c_broad=None,\n",
    "                      drb1_split=None,\n",
    "                      drb1_broad=None,\n",
    "                      unacceptable_antigens=None,\n",
    "                      random_numb_generator = np.random):\n",
    "    \"\"\"\n",
    "    Generates a random HLA haplotype based on a given probability distribution.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): A dataframe representing the probability distribution of HLA haplotypes.\n",
    "    - a (str): The HLA antigen to be filtered.\n",
    "    - b (str): The HLA antigen to be filtered.\n",
    "    - c (str): The HLA antigen to be filtered.\n",
    "    - drb1 (str): The HLA antigen to be filtered.\n",
    "    - random_numb_generator (module): The random number generator to use, defaulting to `np.random`.\n",
    "\n",
    "    Returns:\n",
    "    - list: The randomly generated HLA haplotype.\n",
    "\n",
    "    Notes:\n",
    "    - If filtering criteria result in no matching haplotypes, the function falls back to the original unfiltered DataFrame.\n",
    "    - This is first assessed for unacceptable antigens, and then for HLAs.\n",
    "    - Frequency normalization occurs after filtering to maintain a valid probability distribution for selection.\n",
    "    \"\"\"\n",
    "\n",
    "    df_orig = df.copy(deep=True)\n",
    "    \n",
    "    # Filter out rows where any column contains an antigen from unacceptable_antigens\n",
    "    if not pd.isnull(unacceptable_antigens):\n",
    "        unacceptable_antigens = set(unacceptable_antigens.split())  # Convert to set for faster lookups\n",
    "        df_unacc_antigens_filtered = df[~df[[\"hla_a_split\", \"hla_a_broad\", \"hla_b_split\", \"hla_b_broad\", \"hla_c_split\", \"hla_c_broad\", \"hla_drb1_broad\", \"hla_drb1_split\"]].isin(unacceptable_antigens).any(axis=1)].copy()\n",
    "    else:\n",
    "         df_unacc_antigens_filtered = df.copy()\n",
    "\n",
    "    if len(df_unacc_antigens_filtered[\"normalized_freq\"]) == 0:\n",
    "            print(\"Unnacceptable antigens filtering: The following combination is not represented in the HLA haplotype frequencies DataFrame:\")\n",
    "            print(f\"a split:{a_split}, a broad:{a_broad}, b split:{b_split}, b broad:{b_broad}, c split:{c_split}, c broad:{c_broad}, drb1:{drb1_split}, drb1:{drb1_broad} unnacceptable antigens:{unacceptable_antigens}\")\n",
    "\n",
    "            # some combinations are not in the haplotype frequency data\n",
    "            # in these cases a completly new haplotype is generated\n",
    "            # for this the unfiltered dataframe is used\n",
    "            df_unacc_antigens_filtered = df_orig.copy()\n",
    "\n",
    "    filters = [(\"hla_a_split\", a_split), \n",
    "           (\"hla_a_broad\", a_broad),\n",
    "           (\"hla_b_split\", b_split),\n",
    "           (\"hla_b_broad\", b_broad),\n",
    "           (\"hla_c_split\", c_split),\n",
    "           (\"hla_c_broad\", c_broad),\n",
    "           (\"hla_drb1_split\", drb1_split),\n",
    "           (\"hla_drb1_broad\", drb1_broad)]\n",
    "    \n",
    "    df_hla_filtered = df_unacc_antigens_filtered.copy()\n",
    "    \n",
    "    for col, value in filters:\n",
    "        if not pd.isnull(value):\n",
    "            df_hla_filtered = df_hla_filtered.loc[(df_hla_filtered[col] == value) | (pd.isnull(df_hla_filtered[col]))].copy()\n",
    "\n",
    "    if len(df_hla_filtered[\"normalized_freq\"]) == 0:\n",
    "            print(\"HLA allele filtering: The following combination is not represented in the HLA haplotype frequencies DataFrame:\")\n",
    "            print(f\"a split:{a_split}, a broad:{a_broad}, b split:{b_split}, b broad:{b_broad}, c split:{c_split}, c broad:{c_broad}, drb1 split:{drb1_split}, drb1 broad:{drb1_broad} unnacceptable antigens:{unacceptable_antigens}\")\n",
    "\n",
    "            # some combinations are not in the haplotype frequency data\n",
    "            # in these cases a completly new haplotype is generated\n",
    "            # for this the unfiltered dataframe is used\n",
    "            df_hla_filtered = df_unacc_antigens_filtered\n",
    "\n",
    "    # the frequencies have to be normalized after filtering and the index has to be reset\n",
    "    if any(not pd.isnull(val) for val in [a_broad, b_broad, c_broad, drb1_broad, drb1_split, a_split, b_split, c_split, unacceptable_antigens]):\n",
    "        df_hla_filtered[\"normalized_freq\"] = df_hla_filtered[\"normalized_freq\"]/df_hla_filtered[\"normalized_freq\"].sum()\n",
    "\n",
    "        df_hla_filtered = df_hla_filtered.reset_index(drop=True)\n",
    "\n",
    "    haplotype = df_hla_filtered.iloc[[random_numb_generator.choice(df_hla_filtered.index, p = df_hla_filtered[\"normalized_freq\"])]]\n",
    "    haplotype = haplotype[[\"hla_a_split\", \"hla_a_broad\", \"hla_b_split\", \"hla_b_broad\", \"hla_c_split\", \"hla_c_broad\", \"hla_drb1_split\", \"hla_drb1_broad\"]]\n",
    "\n",
    "    return list(haplotype.values[0])\n",
    "\n",
    "def impute_hla_hapl(df, random_numb_generator):\n",
    "    \"\"\"\n",
    "    Imputes missing HLA antigen values in specific columns by generating random haplotypes.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The DataFrame containing columns with missing HLA antigen values.\n",
    "    - random_numb_generator (module): The random number generator to use.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with missing HLA antigen values imputed in specified columns.\n",
    "\n",
    "    Notes:\n",
    "    - The function applies `gen_hla_haplotype` to impute missing values in two sets of HLA columns (`hla_a_split_1`, `hla_a_broad_1`, etc., and `hla_a_split_2`, `hla_a_broad_2`, etc.).\n",
    "    - Filtering is based on available antigen information in each row, and random HLA haplotypes are generated only for rows with missing values in each respective set of columns.\n",
    "    \"\"\"\n",
    "\n",
    "    df_out = df.copy(deep=True)\n",
    "\n",
    "    necessary_hla_columns_1 = [\"hla_a_broad_1\", \"hla_b_broad_1\", \"hla_c_broad_1\", \"hla_drb1_split_1\"]\n",
    "    hla_columns_1 = [\"hla_a_split_1\", \"hla_a_broad_1\", \"hla_b_split_1\", \"hla_b_broad_1\", \"hla_c_split_1\", \"hla_c_broad_1\", \"hla_drb1_split_1\", \"hla_drb1_broad_1\"]\n",
    "    imputed1 = df.apply(lambda x: gen_hla_haplotype(df_hla_haplo_freq, \n",
    "                                                a_split=x[\"hla_a_split_1\"], \n",
    "                                                a_broad=x[\"hla_a_broad_1\"], \n",
    "                                                b_split=x[\"hla_b_split_1\"],\n",
    "                                                b_broad=x[\"hla_b_broad_1\"],\n",
    "                                                c_split=x[\"hla_c_split_1\"],\n",
    "                                                c_broad=x[\"hla_c_broad_1\"],\n",
    "                                                drb1_split=x[\"hla_drb1_split_1\"],\n",
    "                                                drb1_broad=x[\"hla_drb1_broad_1\"],\n",
    "                                                unacceptable_antigens=x[\"latest_unacceptable_antigens\"],\n",
    "                                                random_numb_generator = random_numb_generator)\n",
    "                    if any(pd.isnull(x[col]) for col in necessary_hla_columns_1)\n",
    "                    else [x[col] for col in hla_columns_1],\n",
    "                    axis=1, result_type=\"expand\")\n",
    "    \n",
    "    necessary_hla_columns_2 = [\"hla_a_broad_2\", \"hla_b_broad_2\", \"hla_c_broad_2\", \"hla_drb1_split_2\"]\n",
    "    hla_columns_2 = [\"hla_a_split_2\", \"hla_a_broad_2\", \"hla_b_split_2\", \"hla_b_broad_2\", \"hla_c_split_2\", \"hla_c_broad_2\", \"hla_drb1_split_2\", \"hla_drb1_broad_2\"]\n",
    "    imputed2 = df.apply(lambda x: gen_hla_haplotype(df_hla_haplo_freq, \n",
    "                                                a_split=x[\"hla_a_split_2\"], \n",
    "                                                a_broad=x[\"hla_a_broad_2\"], \n",
    "                                                b_split=x[\"hla_b_split_2\"],\n",
    "                                                b_broad=x[\"hla_b_broad_2\"],\n",
    "                                                c_split=x[\"hla_c_split_2\"],\n",
    "                                                c_broad=x[\"hla_c_broad_2\"],\n",
    "                                                drb1_split=x[\"hla_drb1_split_2\"],\n",
    "                                                drb1_broad=x[\"hla_drb1_broad_2\"],\n",
    "                                                unacceptable_antigens=x[\"latest_unacceptable_antigens\"],\n",
    "                                                random_numb_generator = random_numb_generator)\n",
    "                    if any(pd.isnull(x[col]) for col in necessary_hla_columns_2)\n",
    "                    else [x[col] for col in hla_columns_2],\n",
    "                    axis=1, result_type=\"expand\")\n",
    "\n",
    "    df_out.loc[:, hla_columns_1] = pd.DataFrame(imputed1).values\n",
    "    df_out.loc[:, hla_columns_2] = pd.DataFrame(imputed2).values\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_random_rng = np.random.default_rng(1234)\n",
    "df_active_waiting_2006 = impute_hla_hapl(df_active_waiting_2006, hla_random_rng)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Testing for cases where the HLA type is in the unacceptible antigens list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active_waiting_2006.loc[df_active_waiting_2006[[\"hla_a_split_1\",\n",
    "                                                   \"hla_a_broad_1\",\n",
    "                                                   \"hla_b_split_1\",\n",
    "                                                   \"hla_b_broad_1\",\n",
    "                                                   \"hla_c_split_1\",\n",
    "                                                   \"hla_c_broad_1\",\n",
    "                                                   \"hla_drb1_split_1\",\n",
    "                                                   \"hla_drb1_broad_1\",\n",
    "                                                   \"hla_a_split_2\",\n",
    "                                                   \"hla_a_broad_2\",\n",
    "                                                   \"hla_b_split_2\",\n",
    "                                                   \"hla_b_broad_2\",\n",
    "                                                   \"hla_c_split_2\",\n",
    "                                                   \"hla_c_broad_2\",\n",
    "                                                   \"hla_drb1_split_2\",\n",
    "                                                   \"hla_drb1_broad_2\"]].isin(df_active_waiting_2006[\"latest_unacceptable_antigens\"]).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Load peak vPRAs and corresponding unacceptable antigens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peak_vpra = pd.read_csv(data_path + \"peak_vPRAs_freq.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "\n",
    "df_peak_vpra[\"unacceptable_antigens\"] = df_peak_vpra[\"unacceptable_antigens\"].apply(lambda x: x.split() if not pd.isnull(x) else x)\n",
    "\n",
    "print(df_peak_vpra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Load Blood group frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_blood_grp_freq_dict = pd.read_csv(data_path + \"donor_blood_grp.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "\n",
    "donor_blood_grp_freq_dict = donor_blood_grp_freq_dict.set_index(\"blood_grp_donor\")[\"frequency\"].to_dict()\n",
    "\n",
    "print(donor_blood_grp_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipient_blood_grp_freq_dict = pd.read_csv(data_path + \"recipient_blood_grp.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "\n",
    "recipient_blood_grp_freq_dict = recipient_blood_grp_freq_dict.set_index(\"blood_grp_rec\")[\"frequency\"].to_dict()\n",
    "\n",
    "print(recipient_blood_grp_freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Load dialysis to registration time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dial_to_reg = pd.read_csv(data_path + \"time_dial_to_reg.csv\", sep = \";\", decimal = \",\", index_col=0)\n",
    "\n",
    "df_dial_to_reg[\"age_at_reg_bins\"] = pd.cut(df_dial_to_reg[\"age_at_reg_waiting_list\"], bins=range(0,101, 10), labels=False)*10\n",
    "\n",
    "df_dial_to_reg.sort_values(by=\"age_at_reg_bins\", inplace=True)\n",
    "\n",
    "print(df_dial_to_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kde_dict_dial_to_reg(df_time_dial_to_reg):\n",
    "    dict_kde_dial_to_reg = {}\n",
    "    for age_bin in df_time_dial_to_reg.age_at_reg_bins:\n",
    "        kde = gaussian_kde(df_time_dial_to_reg.loc[df_time_dial_to_reg[\"age_at_reg_bins\"] == age_bin, \"time_dial_to_registration\"])\n",
    "        dict_kde_dial_to_reg.setdefault(age_bin, kde)\n",
    "    return  dict_kde_dial_to_reg\n",
    "\n",
    "dict_kde_dial_to_reg = make_kde_dict_dial_to_reg(df_dial_to_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde_dial_to_reg(kde_dict, age_bin, ax):\n",
    "    x = np.linspace(-100, 5000, 1000)\n",
    "    y = kde_dict[age_bin](x)\n",
    "    ax.plot(x, y)\n",
    "    ax.set_ylim([0, 0.0025])\n",
    "    ax.fill_between(x, y, alpha=0.5)\n",
    "    ax.set_xlabel('Days')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{age_bin} to {age_bin+10} years')\n",
    "\n",
    "num_plots = len(dict_kde_dial_to_reg.keys())\n",
    "num_cols = 3\n",
    "num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(8,8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each KDE in a separate subplot\n",
    "for idx, age_bin in enumerate(dict_kde_dial_to_reg.keys()):\n",
    "    plot_kde_dial_to_reg(dict_kde_dial_to_reg, age_bin, axs[idx])\n",
    "\n",
    "for idx in range(num_plots, num_rows * num_cols):\n",
    "    fig.delaxes(axs[idx])\n",
    "\n",
    "plt.suptitle(\"KDE for time from dialysis to registration by age at list registration\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Load DSO region frequencies for donors and recipients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_region_freq = pd.read_csv(data_path + \"donor_dso_reg.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "donor_region_freq = donor_region_freq.set_index(\"dso_region_donor\")[\"frequency\"].to_dict()\n",
    "print(donor_region_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipient_region_freq = pd.read_csv(data_path + \"recipient_dso_reg.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "recipient_region_freq = recipient_region_freq.set_index(\"dso_region_rec\")[\"frequency\"].to_dict()\n",
    "print(recipient_region_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Load newly registred patients per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pts_list = pd.read_csv(data_path + \"new_reg_wait_per_year.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "print(new_pts_list)\n",
    "new_pts_list = new_pts_list[\"n\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Load transplantations per year\n",
    "\n",
    "Only post-mortem transplantations are counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_per_year_list = pd.read_csv(data_path + \"tx_per_year.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "print(tx_per_year_list)\n",
    "tx_per_year_list = tx_per_year_list[\"n\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Load frequency of living donor donation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "living_donor_prob = pd.read_csv(data_path + \"freq_living_donor.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "\n",
    "living_donor_prob[\"reg_waiting_list_year\"] = living_donor_prob[\"reg_waiting_list_year\"].str.replace(\"(\", \"\").str.replace(\"]\", \"\")\n",
    "\n",
    "print(living_donor_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "living_donor_prob = pd.read_csv(data_path + \"freq_living_donor.csv\", sep = \";\", decimal = \",\", index_col = 0)\n",
    "\n",
    "# Convert the reg_waiting_list_year column to intervals\n",
    "living_donor_prob[\"reg_waiting_list_year\"] = living_donor_prob[\"reg_waiting_list_year\"].str.replace(\"(\", \"\").str.replace(\"]\", \"\")\n",
    "living_donor_prob[[\"start\", \"end\"]] = living_donor_prob[\"reg_waiting_list_year\"].str.split(\",\", expand=True).astype(str)\n",
    "\n",
    "# Convert to float (handle 'Inf')\n",
    "living_donor_prob[\"start\"] = living_donor_prob[\"start\"].replace(\"0\", \"0\").astype(float)\n",
    "living_donor_prob[\"end\"] = living_donor_prob[\"end\"].replace(\"Inf\", \"inf\").astype(float)\n",
    "\n",
    "# Now build a lookup dictionary:\n",
    "living_donor_prob_lookup = {}\n",
    "\n",
    "for age, row in living_donor_prob.iterrows():\n",
    "    start, end = row[\"start\"], row[\"end\"]\n",
    "    prob = row[\"living_donor_prob\"]\n",
    "    living_donor_prob_lookup[(age, start, end)] = prob\n",
    "\n",
    "living_donor_prob = living_donor_prob_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Load age list for donors and recipients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the csv then taking the second column and coverting it to a list\n",
    "list_pts_age = pd.read_csv(data_path + \"waiting_list_reg_age.csv\", sep = \";\", decimal = \",\").iloc[:,1].tolist()\n",
    "\n",
    "list_donor_age = pd.read_csv(data_path + \"donor_age.csv\", sep = \";\", decimal = \",\").iloc[:,1].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney_dtsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
